> # 第四讲 XTuner 大模型单卡低成本微调实战

## 前言
大语言模型是在海量文本内容上，以无监督或半监督的方式进行训练的模型，它的训练数据集通常包含了大量的文本数据，如维基百科、新闻、书籍、网页等。这些模型在训练过程中，通过学习文本数据中的统计规律，从而学习到了丰富的语言知识，包括语法、语义、逻辑等。

但是，由于训练数据的局限性，大语言模型在实际应用中还存在一些局限性，如知识时效性受限、专业能力有限、定制化成本高等问题。为了解决这些问题，令他能够有更好的应用价值，当下也有两种基本思路解决它，分别是通过Finetune（微调）和RAG（检索增强生成）。

在上一节课程中我们已经学习如何使用RAG范式，利用LangChain解决这一问题，本节课程中我们将学习如何通过Finetune（微调）的方式，来解决大语言模型的局限性。

## 一、Finetune简介
在机器学习领域中，微调（Fine-tuning）是指在一个预训练模型的基础上，通过使用新的数据集或任务来进一步训练模型，以适应新的任务或特定领域的需求。在大型语言模型中，微调是一种常见的技术，用于将通用的预训练语言模型适应到特定任务或领域，以提高模型在该任务或领域上的性能。
微调大型语言模型的过程通常包括以下几个步骤：
- 选择预训练模型： 首先，选择一个适合于特定任务或领域的预训练语言模型。常见的预训练模型包括BERT、GPT、RoBERTa等。
- 准备数据集： 接下来，准备用于微调的数据集，这些数据集通常包括与目标任务或领域相关的文本数据。
- 定义任务和目标： 确定要解决的任务类型，如文本分类、命名实体识别、文本生成等，并定义相应的目标函数。
- 微调模型： 使用预训练模型作为初始参数，将其与新的数据集一起输入到模型中，并通过反向传播算法更新模型参数，使其适应新的任务或领域。
- 评估性能： 在微调完成后，使用独立的验证集或测试集评估模型在目标任务上的性能，并根据评估结果进行调整或优化。

微调大型语言模型的优势在于，它可以利用预训练模型在大规模文本数据上学到的通用语义知识，并在相对较小的数据集上进行额外训练，从而快速有效地提升模型性能。此外，微调还可以避免从头开始训练模型所需的大量计算资源和时间，节省了训练成本。
然后LLM 的下游应用中，增量预训练和指令跟随是经常会用到两种的微调模式。

- 增量预训练
  - 在原有的预训练模型上，继续训练，提供更多数据，以适应特定领域的语言模型。
  - 使用场景:让基座模型学习到一些新知识，如某个垂类领域的常识
  - 训练数据:文章、书籍、代码等

- 指令跟随微调
  - 提供一些指令，让模型学会对话模板，根据人类指令进行对话
  - 使用场景:让模型学会对话模板，根据人类指令进行对话
  - 训练数据:高质量的对话、问答数据

下面将针对两种微调手段进行详细介绍。

### 1. 指令跟随微调
指令跟随微调是为了得到能够实际对话的LLM，介绍指令跟随微调前，需要先了解如何使用LLM进行对话。在实际对话时，通常会有三种角色——
- System：给定一些上下文信息，比如“你是一个安全的 A1助手”
- User：实际用户，会提出一些问题，比如“世界第一高峰是?”
- Assistant：根据 User 的输入，结合 System 的上下文信息，做出回答，比如“珠穆朗玛峰”

### 2. 增量预训练
增量预训练是为了让模型学会一些新的知识，比如某个垂类领域的常识。在增量预训练中，我们需要提供一些领域相关的数据，让模型学会这些领域的知识。而这一过程只有一种角色即Assistant，它会根据提供的领域数据，学会这些领域的知识。

## 二、XTuner简介
XTuner是由书生浦语开发的微调框架，是一个大模型微调工具箱，支持从Hugging Face和Model Scope中加载模型和数据集，支持多款开源大模型，允许让用户在消费级显卡条件下进行大模型微调，免除复杂的数据格式，帮助开发人员专注于数据本身。
